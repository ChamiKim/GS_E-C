{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45563158",
   "metadata": {},
   "source": [
    "# Try 01.\n",
    "- 네이버 카페 크롤러 참고했습니당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0. 필요한 모듈과 라이브러리 로딩\n",
    "from bs4 import BeautifulSoup     \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbf8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling():\n",
    "     for content_num in range(4, 24):\n",
    "        try:\n",
    "           \n",
    "            #--- 제목 수집\n",
    "            time.sleep(random.uniform(0.5,1.2))\n",
    "            title = [driver.find_element(By.CLASS_NAME,'title_subject').text]\n",
    "            #titles.append(title)\n",
    "\n",
    "            #--- 날짜 수집\n",
    "            time.sleep(random.uniform(0.2, 1.3))\n",
    "            date = driver.find_element(By.CLASS_NAME, \"gall_date\").text\n",
    "            dates.append(date)\n",
    "\n",
    "            #--- 본문 수집\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "            content = driver.find_element(By.CLASS_NAME, \"write_div\").text\n",
    "            content = [content.replace(\"\\n\", ' ')]   # '\\n'을 공백으로\n",
    "            #contents.append(content)\n",
    "\n",
    "            #--- 댓글 수집\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "            iscomment = soup.find_all(class_='usertxt ub-word')\n",
    "            if len(iscomment) == 0: \n",
    "                box = []\n",
    "            else:\n",
    "                WebDriverWait(driver, 15).until( EC.presence_of_all_elements_located((By.CLASS_NAME, \"comment_box\")) )\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                iscomment = soup.find_all(class_='usertxt ub-word')\n",
    "                box = []\n",
    "                for i in iscomment:\n",
    "                    box.append(i.get_text())\n",
    "\n",
    "\n",
    "            contents.append(title + content + box)\n",
    "            \n",
    "                        \n",
    "            # 키워드 만들기\n",
    "            keywords.append(keyword)\n",
    "\n",
    "            # 다음 게시물 클릭 및 반복\n",
    "            driver.find_element(By.CSS_SELECTOR, '#bottom_listwrap > section.left_content.result > article > div.gall_listwrap.list > table > tbody > tr:nth-child({0}) > td.gall_tit.ub-word > a.search_list'.format(content_num)).click()\n",
    "            driver.implicitly_wait(3)\n",
    "            \n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422e2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. 제목, 날짜, 본문, 댓글 리스트 만들고 수집을 위해 제목 클릭해서 들어가기\n",
    "# 제목, 날짜, 본문, 댓글(box) 빈 list 생성\n",
    "titles = []\n",
    "dates = []\n",
    "contents = []\n",
    "comments = []\n",
    "keywords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c19da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23048\\3955271208.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23048\\3955271208.py:74: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- 크롤링 키워드 ---------------------------\n",
    "query_txt = ['음쓰', '유지비', '플스' '피규어+레고', '담배냄새', '요리', '쇠질', '취미', '아침밥', '도시락', '빨래','집청소','주차']\n",
    "\n",
    "#Step 1. 크롬 드라이버를 사용해서 웹 브라우저 실행\n",
    "\n",
    "path = \"C:/Users/user/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "url = \"https://gall.dcinside.com/board/lists?id=\"\n",
    "gallery = 'room'\n",
    "\n",
    "'''driver.get(url)   ###+gallery)\n",
    "time.sleep(random.randrange(1,5))  # 2 - 5 초 사이에 랜덤으로 시간 선택 '''\n",
    "    \n",
    "for keyword in query_txt:\n",
    "    \n",
    "    driver.get(url+gallery)   ###+gallery)\n",
    "    time.sleep(random.randrange(1,5)) \n",
    "    \n",
    "    #Step 2. 검색창의 이름을 찾아서 검색어 입력\n",
    "\n",
    "    element = driver.find_element(By.NAME, \"search_keyword\")\n",
    "    # element.send_keys(Keys.CONTROL + \"A\") # Keys.LeftShift + Keys.Home\n",
    "    element.send_keys(keyword)\n",
    "    element.find_element(By.XPATH, '/html/body/div[2]/div[3]/main/section[1]/article[2]/form[2]/fieldset/div/div[2]/button').click()\n",
    "    \n",
    "    # Step 3. 페이지에서 가져올 컨텐츠 찾기\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "            \n",
    "        # Step 5. 첫번째 게시물 클릭 (선택자 입력_여기서는 xpath copy 해줌) 및 데이터 수집 시작\n",
    "\n",
    "        driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/main/section[1]/article[2]/div[3]/table/tbody/tr[3]/td[2]/a[1]').click()\n",
    "        crawling()\n",
    "        \n",
    "        \n",
    "        # Step 6. 마지막 페이지 숫자 찾기\n",
    "\n",
    "        try : # 15페이지가 넘는 경우\n",
    "            driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/main/section/article[3]/div/section[1]/article/div[3]/div[1]/a[16]').click()\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            div = soup.find('div', {'class':'bottom_paging_wrap re'})\n",
    "            em = div.find('em')\n",
    "            last = em.text\n",
    "\n",
    "            page = div.find('a')\n",
    "            link = 'https://gall.dcinside.com'+ page.attrs['href']\n",
    "\n",
    "        except : # 15페이지보다 적은 경우\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            div = soup.find('div', {'class': 'bottom_paging_wrapre'})\n",
    "            a_tag = div.find_all('a')\n",
    "            last = a_tag[-2].text\n",
    "\n",
    "            page = div.find('a')\n",
    "            link = 'https://gall.dcinside.com'+ page.attrs['href']\n",
    "\n",
    "\n",
    "        # Step 7. 이동할 페이지 링크 찾기\n",
    "\n",
    "        links = []\n",
    "        for new_page_number in range(2,int(last)+1):\n",
    "            page_number = link.split('page=')[1].split('&')[0]\n",
    "            new_link = link.replace(f\"page={page_number}\", f\"page={new_page_number}\")\n",
    "            links.append(new_link)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        # Step 8. 다음 페이지들에 대해서도 데이터 수집 시작\n",
    "\n",
    "        for link in links:\n",
    "            path = \"C:/Users/user/chromedriver.exe\"\n",
    "            driver = webdriver.Chrome(path)\n",
    "            driver.get(link)\n",
    "            # 첫번째 게시물 클릭 (선택자 입력_여기서는 xpath copy 해줌) 및 데이터 수집 시작\n",
    "            driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/main/section[1]/article[2]/div[3]/table/tbody/tr[3]/td[2]/a[1]').click()\n",
    "            crawling()\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66174834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===데이터 합치기(크롤링한 데이터를 하나의 dataframe 형태로 바꿔줌)\n",
    "data = {'keyword': keywords,\n",
    "        #'tit' : titles,\n",
    "        'body': [','.join(map(str, content)) for content in contents],\n",
    "        'date' : dates,\n",
    "        }\n",
    "\n",
    "dataDF = pd.DataFrame(data)\n",
    "\n",
    "#csv로 저장\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "dataDF.to_csv(\"%s_%s_20~30대남성3.csv\"%(gallery, today), encoding = \"utf-8-sig\")\n",
    "\n",
    "#print('============ %s페이지 수집 완료 ==============='%(link.split('page=')[1].split('&')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1811014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>음쓰</td>\n",
       "      <td>집안일 줄일수있는 가전 추천좀..,가족 집안일 원래는 내가 좀 했었는데 곧 다른지역...</td>\n",
       "      <td>2023.03.31 18:35:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>음쓰</td>\n",
       "      <td>음쓰냉장고나 미생물처리 써보신분 잇나여,처음 자취해보는데 음식물쓰래기가 안나오는것도...</td>\n",
       "      <td>2023.03.26 13:28:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>음쓰</td>\n",
       "      <td>공용 건조기랑 음쓰봉투 제공되는 집,내가 계약한 집 - dc official App</td>\n",
       "      <td>2023.03.18 09:56:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>음쓰</td>\n",
       "      <td>종량제쓰레기봉투 질문좀,음쓰봉투랑 일반쓰레기봉투 둘다 서울시 내에서 이사가도 쓸 수...</td>\n",
       "      <td>2023.03.16 22:19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>음쓰</td>\n",
       "      <td>룸붕이 이사오고 첫 그리마 등장.,날씨 따뜻해지니 슬슬 원룸 단점 나오네  싱크대에...</td>\n",
       "      <td>2023.03.13 09:55:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>주차</td>\n",
       "      <td>빌라주차난이도,나이트메어  나머지5대의 운명이 단한대에 달려있네 ㅋㅋ - dc of...</td>\n",
       "      <td>2022.08.31 22:42:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>주차</td>\n",
       "      <td>빌라는 주차어케하냐,대부분 2중이던데 맨날 연락오면 빼고 그런거임?,연락해도 안받고...</td>\n",
       "      <td>2022.08.22 19:42:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>주차</td>\n",
       "      <td>오피스텔 지하주차장에 시동키고 있는놈들 뭐냐?,꼭 층마다 한두대씩 시동 켜놓고 있음...</td>\n",
       "      <td>2022.08.22 12:04:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>주차</td>\n",
       "      <td>집보고왓어,오늘도 가야되...  1기 신도시 쪽인데  신축오피 1000/65 구축오...</td>\n",
       "      <td>2022.08.05 09:58:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>주차</td>\n",
       "      <td>나 2종근린 들어가는데 좆댐?,5000/8이고 원룸 전세임  대학생이라 역근처 방이...</td>\n",
       "      <td>2022.08.01 03:48:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    keyword                                               body  \\\n",
       "0        음쓰  집안일 줄일수있는 가전 추천좀..,가족 집안일 원래는 내가 좀 했었는데 곧 다른지역...   \n",
       "1        음쓰  음쓰냉장고나 미생물처리 써보신분 잇나여,처음 자취해보는데 음식물쓰래기가 안나오는것도...   \n",
       "2        음쓰     공용 건조기랑 음쓰봉투 제공되는 집,내가 계약한 집 - dc official App   \n",
       "3        음쓰  종량제쓰레기봉투 질문좀,음쓰봉투랑 일반쓰레기봉투 둘다 서울시 내에서 이사가도 쓸 수...   \n",
       "4        음쓰  룸붕이 이사오고 첫 그리마 등장.,날씨 따뜻해지니 슬슬 원룸 단점 나오네  싱크대에...   \n",
       "..      ...                                                ...   \n",
       "180      주차  빌라주차난이도,나이트메어  나머지5대의 운명이 단한대에 달려있네 ㅋㅋ - dc of...   \n",
       "181      주차  빌라는 주차어케하냐,대부분 2중이던데 맨날 연락오면 빼고 그런거임?,연락해도 안받고...   \n",
       "182      주차  오피스텔 지하주차장에 시동키고 있는놈들 뭐냐?,꼭 층마다 한두대씩 시동 켜놓고 있음...   \n",
       "183      주차  집보고왓어,오늘도 가야되...  1기 신도시 쪽인데  신축오피 1000/65 구축오...   \n",
       "184      주차  나 2종근린 들어가는데 좆댐?,5000/8이고 원룸 전세임  대학생이라 역근처 방이...   \n",
       "\n",
       "                    date  \n",
       "0    2023.03.31 18:35:51  \n",
       "1    2023.03.26 13:28:16  \n",
       "2    2023.03.18 09:56:22  \n",
       "3    2023.03.16 22:19:59  \n",
       "4    2023.03.13 09:55:14  \n",
       "..                   ...  \n",
       "180  2022.08.31 22:42:48  \n",
       "181  2022.08.22 19:42:10  \n",
       "182  2022.08.22 12:04:53  \n",
       "183  2022.08.05 09:58:43  \n",
       "184  2022.08.01 03:48:32  \n",
       "\n",
       "[185 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3b4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d658e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8164cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
